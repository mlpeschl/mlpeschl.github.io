<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>POMDP | Markus Peschl</title>
    <link>https://mlpeschl.com/tag/pomdp/</link>
      <atom:link href="https://mlpeschl.com/tag/pomdp/index.xml" rel="self" type="application/rss+xml" />
    <description>POMDP</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Fri, 06 Nov 2020 15:23:13 +0100</lastBuildDate>
    <image>
      <url>https://mlpeschl.com/images/icon_hu6a98d456223992a97ada131f146ee6ab_65108_512x512_fill_lanczos_center_2.png</url>
      <title>POMDP</title>
      <link>https://mlpeschl.com/tag/pomdp/</link>
    </image>
    
    <item>
      <title>ADRQN-PyTorch: A Torch implementation of the action-specific deep recurrent Q network.</title>
      <link>https://mlpeschl.com/post/tiny_adrqn/</link>
      <pubDate>Fri, 06 Nov 2020 15:23:13 +0100</pubDate>
      <guid>https://mlpeschl.com/post/tiny_adrqn/</guid>
      <description>&lt;h2 id=&#34;before-you-read-source-code&#34;&gt;Before you read (source code)&lt;/h2&gt;
&lt;p&gt;An open source Jupyter notebook including source code for this blog post is available. Click the link below to open an interactive version of the source code in Google Colab.&lt;/p&gt;
&lt;p&gt;Note: Enable GPU usage in the runtime settings for training with cuda.
&lt;a href=&#34;https://colab.research.google.com/github/mlpeschl/stat-notebooks/blob/master/Model_Free_RL/ADRQN.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Deep Q networks have proven to be an easy to implement method for solving control problems in both continuous or large discrete state spaces. For instance, a highly cited paper by a group of Google Deepmind researchers [1] proposes the use of a deep (convolutional) neural network for approximating the optimal Q function in Atari games. Since then, numerous improvements to the deep Q network (DQN) algorithm have emerged, one notable example being the Rainbow agent [2], which combines fruitful approaches from different subfields of reinforcement learning, including distributional RL, multi-step targets and dueling networks.&lt;/p&gt;
&lt;p&gt;While the above efforts to improve upon the performance of DQNs have proven to be effective, most of the research focuses on fully observable environments, or at least introduces workarounds to overcome partial observability by combining multiple consecutive frames into a single observation [1]. If such design choices are not possible, one needs a different mechanism for keeping track of the history of observations when approximating the Q function with a neural network. A rather straight forward way to achieve this, is by including recurrence in the Q network, and this is exactly what the action-specific deep recurrent Q network (ADRQN) does [3].&lt;/p&gt;
&lt;p&gt;In this post, we will go through a simple PyTorch implementation of the ADRQN including a small scale experiment on classical control tasks of the OpenAI gym.&lt;/p&gt;
&lt;h1 id=&#34;model-architecture&#34;&gt;Model Architecture&lt;/h1&gt;
&lt;p&gt;The ADRQN [3] is a modification of the DQN algorithm, which introduces an intermediate LSTM layer for remembering action-observation pairs when dealing with partial observability. An outline of the originally proposed architecture can be seen below in figure 1.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;em&gt;Figure 1 (taken from the original paper [3]): ADRQN architecture&lt;/em&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;adrqn_architecture.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;At each time step $t$ the model takes an observation $o_{t}$ and the action that led to that observation $a_{t-1}$, as well as the hidden state $h_{t-1}$ from the last forward pass of the LSTM layer. The observation $o_t$ gets fed through a series of convolutional downsampling layers, whereas the action $a_{t-1}$ gets embedded in a higher dimensional space through a fully connected linear layer (named IP in the figure). Then, the downsampled observation and embedded action get concatenated and fed into a LSTM, which updates its hidden state $h_t$ and outputs a sequence that gets fed into another linearly connected layer. The output size of the final linear layer matches the number of actions to approximate Q values for each possible action.&lt;/p&gt;
&lt;p&gt;Since we will be testing this architecture on low-dimensional physics based state vectors of the classical control tasks in the OpenAI gym, we replace the convolutional layers with linear layers and ReLU activations. Furthermore, we implement an act function into the model class for executing epsilon greedy actions given an action-observation pair.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class ADRQN(nn.Module):
    def __init__(self, n_actions, state_size, embedding_size):
        super(ADRQN, self).__init__()
        self.n_actions = n_actions
        self.embedding_size = embedding_size
        self.embedder = nn.Linear(n_actions, embedding_size)
        self.obs_layer = nn.Linear(state_size, 16)
        self.obs_layer2 = nn.Linear(16, 32)
        self.lstm = nn.LSTM(input_size = 32 + embedding_size, hidden_size = 128, batch_first = True)
        self.out_layer = nn.Linear(128, n_actions)

    def forward(self, observation, action, hidden = None):
        #Takes observations with shape (batch_size, seq_len, state_size)
        #Takes one_hot actions with shape (batch_size, seq_len, n_actions)
        action_embedded = self.embedder(action)
        observation = F.relu(self.obs_layer(observation))
        observation = F.relu(self.obs_layer2(observation))
        lstm_input = torch.cat([observation, action_embedded], dim = -1)
        if hidden is not None:
            lstm_out, hidden_out = self.lstm(lstm_input, hidden)
        else:
            lstm_out, hidden_out = self.lstm(lstm_input)

        q_values = self.out_layer(lstm_out)
        return q_values, hidden_out

    def act(self, observation, last_action, epsilon, hidden = None):
        q_values, hidden_out = self.forward(observation, last_action, hidden)
        if np.random.uniform() &amp;gt; epsilon:
            action = torch.argmax(q_values).item()
        else:
            action = np.random.randint(self.n_actions)
        return action, hidden_out
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If no hidden state is provided to the forward pass, then we omit specifying the hidden argument of the LSTM layer, which equates to setting the hidden state to a zero tensor.&lt;/p&gt;
&lt;h2 id=&#34;sequential-experience-buffer&#34;&gt;Sequential Experience Buffer&lt;/h2&gt;
&lt;p&gt;Besides the model, it is common for DQN type algorithms to use an experience buffer which stores past transitions and provides batches of random transitions for learning the network parameters. Since we are dealing with a recurrent network, we would like to sample batches of sequences at random, on which we then unroll the LSTM to provide a batch of Q-value predictions. While there are various ways to implement such a buffer, we use a simple list that (in order) stores all visited transition tuples $(a_{t-1}, o_t, r_t, a_t, o_{t+1}, d_{t+1})$, where $r_t$ is the reward gained after executing action $a_t$ in observation $o_t$ and $d_{t+1}$ is true if $o_{t+1}$ is a terminal state or otherwise false.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class ExpBuffer():
    def __init__(self, max_storage, sample_length):
        self.max_storage = max_storage
        self.sample_length = sample_length
        self.counter = -1
        self.filled = -1
        self.storage = [0 for i in range(max_storage)]

    def write_tuple(self, aoarod):
      if self.counter &amp;lt; self.max_storage-1:
          self.counter +=1
          if self.filled &amp;lt; self.max_storage-1:
            self.filled += 1
      else:
          self.counter = 0
      self.storage[self.counter] = aoarod

    def sample(self, batch_size):
        #Returns tensors of sizes (batch_size, seq_len, *) where * depends on action/observation/return/done
        seq_len = self.sample_length
        last_actions = []
        last_observations = []
        actions = []
        rewards = []
        observations = []
        dones = []

        for i in range(batch_size):
            if self.filled - seq_len &amp;lt; 0 :
                raise Exception(&amp;quot;Reduce seq_len or increase exploration at start.&amp;quot;)
            start_idx = np.random.randint(self.filled-seq_len)
            last_act, last_obs, act, rew, obs, done = zip(*self.storage[start_idx:start_idx+seq_len])
            last_actions.append(list(last_act))
            last_observations.append(last_obs)
            actions.append(list(act))
            rewards.append(list(rew))
            observations.append(list(obs))
            dones.append(list(done))

        return torch.tensor(last_actions).cuda(), torch.tensor(last_observations, dtype = torch.float32).cuda(), torch.tensor(actions).cuda(), torch.tensor(rewards).float().cuda() , torch.tensor(observations, dtype = torch.float32).cuda(), torch.tensor(dones).cuda()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that with this implementation, we allow the buffer to return sequences that stretch over multiple episodes. If you want to try a buffer that restricts to sampling sequences from a single episode, check out the full source code which includes such an alternative buffer.&lt;/p&gt;
&lt;h2 id=&#34;training-the-network&#34;&gt;Training the Network&lt;/h2&gt;
&lt;p&gt;In the main loop, we sample batches of sequences (with a fixed length) from the experience buffer after each time an action gets executed. To stabilize training, we employ a standard trick of splitting the task of predicting and evaluating Q values into two separate networks, a target network with parameters $\theta^-$ and the main network with parameters $\theta$. For each batch of transition sequences $(a_{t-1}, o_t, r_t, a_t, o_{t+1}, d_{t+1})$ we then compute the $Q$ value with respect to the target network&lt;/p&gt;
&lt;p&gt;$$y_j = r_j + \gamma \cdot (1-\text{is_done}(o_{t+1}))\cdot
\max_{a}Q(h_j, a_j, o_{j+1}, a; \theta^-),$$&lt;/p&gt;
&lt;p&gt;where $\text{is_done}(o_{t+1}) = 1$ if $o_{t+1}$ is a terminal state and $0$ otherwise. Subsequently, we update the parameters of our main $Q$ network via the gradient of the quadratic loss&lt;/p&gt;
&lt;p&gt;$$ \nabla_\theta (y_j - Q(h_{j-1}, a_{j-1}, o_j, a_j; \theta))^2.
$$&lt;/p&gt;
&lt;p&gt;In the implementation, we made use of the Adam optimizer for taking these gradient updates. At the end of each episode we update the target network parameters $\theta^- := \theta$. Furthermore, we give the agent some time to explore and fill the experience buffer before updating the networks.&lt;/p&gt;
&lt;p&gt;Besides that, we employ an $\epsilon$-greedy policy with respect to the $Q$ network and an adaptive exploration schedule for decreasing $\epsilon$ over time. To induce partial observability, we also introduce a blinding probability parameter which sets incoming observations to a zero tensor with a specified probability.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;env = gym.make(&#39;MountainCar-v0&#39;)
state_size = env.observation_space.shape[0]
n_actions = env.action_space.n
embedding_size = 8
M_episodes = 2500
replay_buffer_size = 100000
sample_length = 20
replay_buffer = ExpBuffer(replay_buffer_size, sample_length)
batch_size = 64
eps_start = 0.9
eps = eps_start
eps_end = 0.05
eps_decay = 200
gamma = 0.999
learning_rate = 0.005
blind_prob = 0
EXPLORE = 300

adrqn = ADRQN(n_actions, state_size, embedding_size).cuda()
adrqn_target = ADRQN(n_actions, state_size, embedding_size).cuda()
adrqn_target.load_state_dict(adrqn.state_dict())

optimizer = torch.optim.Adam(adrqn.parameters(), lr = learning_rate)

for i_episode in range(M_episodes):
    done = False
    hidden = None
    last_action = 0
    last_observation = env.reset()
    for t in count():
        action, hidden = adrqn.act(torch.tensor(last_observation).float().view(1,1,-1).cuda(), F.one_hot(torch.tensor(last_action), n_actions).view(1,1,-1).float().cuda(), hidden = hidden, epsilon = eps)
        observation, reward, done, info = env.step(action)

        if np.random.rand() &amp;lt; blind_prob:
            #Induce partial observability
            observation = np.zeros_like(observation)

        reward = np.sign(reward)
        replay_buffer.write_tuple((last_action, last_observation, action, reward, observation, done))

        last_action = action
        last_observation = observation

        #Updating Networks
        if i_episode &amp;gt; EXPLORE:
                #Update exploration parameter
                eps = eps_end + (eps_start - eps_end) * math.exp((-1*(i_episode-EXPLORE))/eps_decay)
                #Sample a batch of action/observation/reward sequences
                last_actions, last_observations, actions, rewards, observations, dones = replay_buffer.sample(batch_size)

                #Pass the sequence of last observations and actions through the network
                q_values, _ = adrqn.forward(last_observations, F.one_hot(last_actions, n_actions).float())
                #Get the q_values for the executed actions in the respective observations
                q_values = torch.gather(q_values, -1, actions.unsqueeze(-1)).squeeze(-1)
                #Query the target network for Q value predictions
                predicted_q_values, _ = adrqn_target.forward(observations, F.one_hot(actions, n_actions).float())
                #Compute Q update target
                target_values = rewards + (gamma * (1 - dones.float()) * torch.max(predicted_q_values, dim = -1)[0])
                #Updating network parameters
                optimizer.zero_grad()
                loss = torch.nn.MSELoss()(q_values , target_values.detach())
                loss.backward()
                optimizer.step()

        if done:
            break

    #Update the target network
    adrqn_target.load_state_dict(adrqn.state_dict())
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;results&#34;&gt;Results&lt;/h1&gt;
&lt;p&gt;This section shows some learning results on the MountainCar-v0 and CartPole-v1 tasks of the OpenAI gym. The main point here is to show that the network works and not to discuss computational/sample efficiency. However, the authors of the ADRQN paper [3] state that the ADRQN outperforms other state of the art DQN variants in partially observable environments.&lt;/p&gt;
&lt;h2 id=&#34;cartpole-v1&#34;&gt;CartPole-v1&lt;/h2&gt;
&lt;p&gt;In the cartpole environment, the goal of the agent is learning a policy that manages to balance a pole which is attached to a cart moving along a frictionless track. The observation consists of a $4$-dimensional array, which includes the position and velocity of the cart and the pole respectively.
The rewards directly correspond to the time that the pole can be balanced and provide $+1$ for each time step. OpenAI considers this environment solved when the agent receives an average return of $195$ or higher over $100$ consecutive trials.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;cartpole1.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;We test two different scenarios in this environment, one where the agent has full access to the observations and one where the observations drop out with a probability of $\frac{1}{2}$. Although training of our shallow ADRQN version is quite fast, we give the agent $300$ time steps to explore before updating the parameters. Figure 2 and 3 show the undiscounted reward and exploration scheme of the agent in the fully observable and partially observable cases respectively. While in the fully observable case, the agent learns a near optimal policy (reaching the maximum of $500$ time steps) after around 100 episodes of learning (with 300 episodes of stored transitions), learning an optimal policy in the partial observable case takes around twice as many learning iterations. Although the learning process is significantly more noisy in the partially observable case, the agent still manages to learn a policy that somewhat consistently achieves the maximum return.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;em&gt;Figure 2: ADRQN learning in Cartpole-v1 (fully observable)&lt;/em&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;Cartpole_ADRQN_MDP.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;em&gt;Figure 3: ADRQN learning in Cartpole-v1 with 0.5 observation censoring probability&lt;/em&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;Cartpole_ADRQN_halfblind.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;mountaincar-v0&#34;&gt;MountainCar-v0&lt;/h2&gt;
&lt;p&gt;The mountaincar environment is slightly more challenging due to highly sparse rewards. The agent receives a reward of $-1$ at each timestep and is given $200$ time steps to reach the flag at the top of the mountain. Without any modifications to the algorithm, the $\epsilon$-greedy exploration scheme takes quite a long time to discover where to go with the car. The observations consist of a $2$-dimensional array including position and velocity, whereas the action space is discrete and consists of pushing the car to the left, right or not at all. OpenAI considers this environment solved when the agent gets a reward of at least -110.0 over 100 consecutive trials.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;mountaincar.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Figures 4 and 5 show the learning progress in the fully observable and partially observable cases respectively. We did not train the agent until the environment was solved. However, we can see that (after an initial exploration phase of 300 episodes) the agent learns to consistently reach a reward of around $-110$ and $-125$ in the fully observable and partially observable cases respectively.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;em&gt;Figure 4: ADRQN learning in MountainCar-v0 (fully observable)&lt;/em&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;mountaincar00.PNG&#34; alt=&#34;&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;em&gt;Figure 5: ADRQN learning in MountainCar with 0.5 observation censoring probability&lt;/em&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;mountaincar05_1.PNG&#34; alt=&#34;&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] Volodymyr Mnih, Kavukcuoglu Koray, Silver David, Rusu Andrei A, Veness Joel, Belle-mare Marc G, Graves Alex, Riedmiller Martin, Fidje-land Andreas K, Ostrovski Georg, et al. &lt;em&gt;Human-level control through deep reinforcement learning&lt;/em&gt;. In: Nature, 518 (7540): 529–533, 2015.&lt;/p&gt;
&lt;p&gt;[2] M. Hessel, J. Modayil, H. Van Hasselt, T. Schaul, G. Ostrovski, W. Dabney, D. Horgan, B. Piot,M. Azar, and D. Silver. &lt;em&gt;Rainbow: Combining improvements in deep reinforcement learning.&lt;/em&gt; In: Thirty-Second AAAI Conference on Artificial Intelligence, 2018&lt;/p&gt;
&lt;p&gt;[3] Zhu, P., Li, X., Poupart, P., and Miao, G. &lt;em&gt;On improving deep reinforcementlearning for pomdps.&lt;/em&gt; arXiv preprint arXiv:1804.06309 (2018).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Making Sense of Particle Filtering for POMDPs (Part 1)</title>
      <link>https://mlpeschl.com/post/particle_filtering_1/</link>
      <pubDate>Tue, 20 Oct 2020 20:02:56 +0200</pubDate>
      <guid>https://mlpeschl.com/post/particle_filtering_1/</guid>
      <description>&lt;h2 id=&#34;before-you-read-source-code&#34;&gt;Before you read (source code)&lt;/h2&gt;
&lt;p&gt;An open source Jupyter notebook including source code for this blog post is available. Click the link below to open an interactive version of the source code in your browser.
&lt;a href=&#34;https://mybinder.org/v2/gh/mlpeschl/stat-notebooks/master?filepath=POMDP_Particle_Filters%2Fparticle%20filtering.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;https://mybinder.org/badge_logo.svg&#34; alt=&#34;Binder&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In the field of reinforcement learning, partially observable Markov decision processes (POMDPs) form a mathematical model for the underlying decision process of an agent that is unable to know the exact state it is currently in. While incorporating uncertainty about the state of the environment complicates matters mathematically, it allows for modeling more general problems that arise in various real world problems. Take, for example, a robot which has to navigate through its environment by using sensor data. Such data is inevitably noisy, which gives rise to an inherent uncertainty about the exact state that the robot is currently in.
Even in discrete environments, like Atari games (which have become a standard task in deep reinforcement learning research), a single frame sometimes fails to convey the exact state of the environment, since variables depending on the rate of change (e.g. velocity of an enemy) can only be inferred through multiple consecutive observations.&lt;/p&gt;
&lt;p&gt;In this blog post we will explore a popular method for tracking the unobservable state in a POMDP, called particle filtering, with a simple example of recovering noisy observations in a discrete time Markov process.&lt;/p&gt;
&lt;h2 id=&#34;pomdps-and-beliefs&#34;&gt;POMDPs and Beliefs&lt;/h2&gt;
&lt;p&gt;Formally, a POMDP consists of a tuple $\langle S, A, \Omega, T, O, R \rangle$, where $S$ is the state space, $A$ is the set of actions and $\Omega$ denotes the set of possible observations. A transition function $T(s&#39;|s,a)$ gives the probability of transitioning to state $s&#39;$ when in state $s \in S$ and executing action $a \in A$, whereas an observation function $O(o | s&#39;,a)$ gives the probability of observing observation $o \in \Omega$ after action $a$ has taken the agent to state $s&#39;$. Finally, $R(s,a)$ gives the reward when taking action $a$ in state $s$. The goal (as in the fully observable Markov decision process), is usually to maximize an expected long-term reward of the form
$$\mathbb{E}\left[\sum_{t=0}^h \gamma^t R_t\right],$$
where $0 \leq \gamma \leq 1$ is a discount factor, $h$ is the planning horizon and $R_t$ is the reward obtained at timestep $t$.&lt;/p&gt;
&lt;p&gt;To tackle the problem of partial observability, one needs to incorporate some kind of memory into the agents decision process. While tracking a history of past events is one possibility, another approach is to summarize this into a sufficient statistic, called the belief state $b(s)$. The belief state is a probability distribution over all possible states and models the agents believes about the current true state of the environment. Although the exact computation of $b$ with respect to an initial prior belief $b_0$ when observing $o$ after action $a$ is possible through Bayesian updating, this requires knowledge of the POMDP dynamics:&lt;/p&gt;
&lt;p&gt;\begin{equation}b_{a,o}(s&#39;) = \frac{O(o|s&#39;,a)\sum_{s\in S} T(s&#39;|s,a)b(s)}{\sum_{s&#39; \in S}O(o|s&#39;,a)\sum_{s\in S}T(s&#39;|s,a)b(s)}.\end{equation}&lt;/p&gt;
&lt;p&gt;Even if the model is known, updating can turn out to be cumbersome due to computational complexity that quadratically grows with the size of the state space $|S|$. While there are various ways to work around this issue, the theory of particle filtering turns out to be  effective and compatible with the POMDP framework for tracking belief states in an approximate manner.&lt;/p&gt;
&lt;h2 id=&#34;particle-filtering-with-sequential-importance-sampling&#34;&gt;Particle Filtering with Sequential Importance Sampling&lt;/h2&gt;
&lt;p&gt;Particle filtering, also known as Sequential Monte Carlo, is a widely used technique for estimating a noisy and/or unobserved signal in a dynamical system. To be precise, we assume that we have a discrete time Markov process ${X_t}$ ($t \in \mathbb{N}$) governed by dynamics $p(x_t | x_{t-1})$  with initial state distribution $p(x_0)$ (for simplicity, we for now drop the possibility of taking actions in the following example). Furthermore, we assume a model for the observations $Y_t$, which is given by $p(y_t | x_t)$, where the values of $Y_t$ are conditionally independent given the values of $X_t$. The general goal of particle filtering is to then approximate the posterior distribution $p(x_t | y_0, y_1, \dots, y_t)$, i.e. to obtain a probability distribution for the true signal at time $t$, given all the information (observations) up to time $t$.&lt;/p&gt;
&lt;p&gt;While this problem has been studied in various fields for a long time and the mathematical theory behind its possible solutions are rich, we will focus on a simple, but effective Monte Carlo algorithm based on importance sampling. Let&amp;rsquo;s assume we are given a noisy observation $(y_0, y_1, \dots, y_{T-1})$ of the signal $(x_0, x_1, \dots, x_{T-1})$ with length $T$. To recover the signal, consider the following procedure (assuming knowledge of $p(x_t | x_{t-1})$ and $p(y_t | x_t)$ ):&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Initialize $N$ sequences $x_i^n$ $(i = 0, \dots , T-1)$ with initial values $x_0^n$ drawn according to $p(x_0)$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Initialize an array of weights $\mathbf{w_0} \in \mathbb{R}^N$ with ones.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For $t = {1,\dots, T-1}$:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Predict the next value for each sequence by sampling from the model:
$$x_t^i \sim p(x| x_{t-1}^i)$$&lt;/li&gt;
&lt;li&gt;For each sequence $x_i^n$ ($i \leq t$), evaluate how likely the observation $y_t$ would have been, assuming that $x_i^n$ was the true signal:
$$w_t^n := w_{t-1}^n \cdot p(y_t | x_{t-1}^n)$$&lt;/li&gt;
&lt;li&gt;Normalize the weights:
$$w_t^n = \frac{w_t^n}{\sum_{i=1}^n w_i^n}$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;By following this simple algorithm, we obtain $N$ sequences (also called &lt;em&gt;particles&lt;/em&gt;) $x_{0:T-1}^n \in \mathbb{R}^{T}$ with corresponding likelihoods $w_{T-1}^n$ that indicate, how likely it was for sequence $x_{0:T-1}^n$ to be the true sequence that led to the observation vector $(y_0, y_1, \dots, y_{T-1})$. Intuitively, if $N$ is chosen large enough, we would expect to find a sequence $x_{0:T-1}^n$ with a high enough weight $w^n$, such that it is a good candidate solution $x_{0:T-1}^n \approx (x_1, x_2, \dots, x_{T-1})$. However, there is a small technical issue with the algorithm above, called &lt;em&gt;particle degeneracy&lt;/em&gt;. Due to randomness, most of the particles $x_{0:t}^n$ will receive a very low weight $w_t^n$ for growing $t$, since propagation of deviations in the generation of $x_t^n$ from the true signal can grow arbitrarily large. Therefore, it can happen that almost all weights will be close to $0$ after the normalization step, while one weight of a particle that most closely resembles the true signal will get all the credit. To avoid this, we can completely resample $N$ particles from the sequences $x_{0:t}^n$ from time to time according to their weights $w_t^n$ and reset the weights $w_t^n = \frac{1}{N}$. Over time, this will discard highly unlikely sequences and only keep promising ones.&lt;/p&gt;
&lt;p&gt;To see this algorithm in action, consider the following example: Let $X_t | X_{t-1} = x_{t-1} \sim \mathcal{N}(x_{t-1} +1, 16)$ be a discrete time random walk with constant upwards drift. Furthermore, let $Y_t | X_t = x_t \sim \Gamma(8,\sqrt{|x_t|})$ follow a Gamma distribution with constant shape parameter.&lt;/p&gt;
&lt;p&gt;We start off our simulation experiment by generating a sequence $(x_0, \dots, x_{49})$ of length $T = 50$ and sample an observation signal $(y_0, \dots, y_{49})$ accordingly. Then, we run the algorithm above with particle resampling using a total of $N = 2000$ particles. Figure 1 shows the resulting posterior mean as well as a level 0.99 confidence band obtained from the empirical particle distribution.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;em&gt;Figure 1: Simulation Study&lt;/em&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;SIS_example.png&#34; alt=&#34;space-1.jpg&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Despite the high noise in the observation, our simple algorithm manages to recover the true signal reasonably well in the mean. Additionally, since we approximate a complete posterior distribution we can make use of the uncertainty quantification that comes with it. This can be particularly useful in reinforcement learning, since the study of decision making inherently involves quantifying ones certainty in particular events.&lt;/p&gt;
&lt;h2 id=&#34;applications-to-pomdps&#34;&gt;Applications to POMDPs&lt;/h2&gt;
&lt;p&gt;While in the example above we excluded the possibility of taking actions in the Markov process $X_t$, particle filtering can be easily extended to the POMDP setting. In fact, sequential importance sampling is a useful tool for approximating the belief state $b(s)$, which in itself can be interpreted as the posterior distribution of the true state given past observations (including actions). For instance, if we had the POMDP dynamics available, we could apply the above algorithm in a slightly modified way:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Instead of sampling $x_t^i \sim p(x| x_{t-1}^i)$ we predict a next candidate state given action $a$ and current candidate state $s$:
$$s&#39; \sim T(\cdot | s,a)$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;After taking action $a$ and observing $o$, we then update our weights $w$ according to the observation likelihood given $a$ and $s&#39;$:
$$w&#39; = O(o| s&#39;, a)w$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This way, we could effectively keep track of likely state sequences $(s_0, s_1, \dots,)$ without ever needing to explicitly observe them.&lt;/p&gt;
&lt;p&gt;It is important to keep in mind that the sequential importance sampling algorithm in the form it is presented above requires explicit knowledge of the state transition and observation model. In case these are available, particle filtering is a useful tool for approximating the exact posterior update $b_{a,o}(s&#39;)$. For example, in Partially Observable Monte-Carlo Planning, an extension of Monte-Carlo Tree Search for POMDPs [1], particle filtering is used to track the belief state while unrolling episodes through a generative model of the POMDP dynamics. When a model is not available, recent deep learning based approaches have been proposed, like deep variational reniforcement learning for POMDPs [2], which learns a deep generative model of the POMDP transition dynamics by sampling likely candidate state sequences through particle filtering. Similarly, particle filters have been used for keeping belief states in Bayesian reinforcement learning for large POMDPs which represent a distribution over possible POMDP models [3].&lt;/p&gt;
&lt;p&gt;Alright, that&amp;rsquo;s it for now. In Parts 2 and 3 of this series, we will dive into more detail of applying particle filtering to POMDP control problems by using methods like the Bayes-Adaptive and deep variational frameworks.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] Silver,D. and Veness, J. Monte-Carlo planning in large POMDPs. In &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt;, pages 2164-2172, 2010.&lt;/p&gt;
&lt;p&gt;[2] Igl, M., Zintgraf, L., Le, T. A., Wood, F., and Whiteson, S. &lt;em&gt;Deep variational reinforcement learning for pomdps&lt;/em&gt;. arXiv:1806.02426, 2018.&lt;/p&gt;
&lt;p&gt;[3] Katt, S., Oliehoek, F. A., and Amato, C. Learning in pomdps with monte carlo tree search.  In &lt;em&gt;International Conference on Machine Learning&lt;/em&gt;, pp. 1819–1827, 2017.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
